<!DOCTYPE html>
<html>

<head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Live Face Detector</title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<style>
		.error-msg {
			color: red;
		}
	</style>
</head>

<body>
	<div class="main-container">
		<video width="0" height="0" autoplay></video>
		<canvas width="640" height="480"></canvas>
		<div class="error-msg"></div>
	</div>
	<script>
		const errorMsg = document.querySelector('.error-msg');
		if ('FaceDetector' in window) {

			// Initializing vars & consts
			const video = document.querySelector('video');
			const canvas = document.querySelector('canvas');
			const context = canvas.getContext('2d');

			const faceDetector = new FaceDetector();
			//The path to the image that we want to add.
			const imgPath = './MargeSimpson15.png';
			// 417x658
			//Create a new Image object.
			const imgObj = new Image();

			let facePics = [];

			//Set the src of this Image object.
			imgObj.src = imgPath;
			// Get access to the camera
			if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
				navigator.mediaDevices.getUserMedia({ video: true }).then(function (stream) {
					video.srcObject = stream;
					video.play();
				});
			}

			async function detect() {
				const faces = await faceDetector.detect(canvas);
				drawFaces(faces);
			}

			// Draw face highlighters
			async function drawFaces(faces) {
				facePics = [];

				faces.forEach(face => {

					const { width, height, top, left } = face.boundingBox;
					console.log(face);

					context.strokeStyle = '#00FF00';
					context.lineWidth = 5;
					//context.strokeRect(left, top, width, height);
					let picWidth = width * 1.5;
					let picHeight = picWidth * (658 / 417);
					let leftOffset = left - width / 4;
					let topOffset = top - 5 / 6 * picHeight;

					facePics.push({
						picWidth: picWidth,
						picHeight: picHeight,
						leftOffset: leftOffset,
						topOffset: topOffset,
					});
					
					//face.landmarks.forEach(landmark => {
					//	context.lineWidth = 1;
					//	if (landmark.type === 'eye') {
					//		context.strokeStyle = '#FF0000';
					//		context.strokeRect(landmark.locations[0].x - 20, landmark.locations[0].y - 20, 40, 20);
					//	} else {
					//		context.strokeStyle = '#0000FF';
					//		context.strokeRect(landmark.locations[0].x - 40, landmark.locations[0].y - 40, 80, 40);
					//	}
					//});
				});
			}

			setInterval(() => {
				context.drawImage(video, 0, 0, canvas.width, canvas.height);
				facePics.forEach( face => {
					context.drawImage(imgObj, face.leftOffset, face.topOffset, face.picWidth, face.picHeight);
				});
				detect();
			}, 100);
		} else {

			// Find if user is using Chrome, and if yes, check version number. Face Detection API has been in the browser since M57
			const chromeVer = /Chrome\/(\d+)/.exec(navigator.userAgent);
			if (chromeVer && parseInt(chromeVer[1]) > 56) {
				errorMsg.textContent = "Looks like your version of Chrome supports FaceDetector API but you haven't enabled it (since experimental features in Chrome are disabled by default). You can enable it by going to the URL 'chrome://flags#enable-experimental-web-platform-features' and enabling the 'Experimental Web Platform features'"
			} else {
				errorMsg.textContent = "Looks like your browser does not support FaceDetector API. If you're on Chrome, please make sure you've the latest update and enable the #enable-experimental-web-platform-features flag from chrome://flags";
			}
		}
	</script>
</body>

</html>